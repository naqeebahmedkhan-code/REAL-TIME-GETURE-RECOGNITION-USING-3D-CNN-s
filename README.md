# REAL-TIME-GETURE-RECOGNITION-USING-3D-CNN-s
âœ‹ Hand Gesture Recognition & Control System
A computer vision-based real-time hand gesture recognition system that allows users to control applications (like switching browser tabs) using hand gestures. Built using OpenCV, MediaPipe, TensorFlow/Keras, and Flask.

ğŸ“Œ Features
ğŸ¥ Real-time hand gesture detection using webcam

ğŸ§  CNN-based gesture recognition (trained on custom data)

âœ‹ Hand tracking using MediaPipe

ğŸ“Š Custom training pipeline for gesture classification

ğŸŒ Flask web app to visualize detection in the browser

ğŸ” Gesture-to-action mapping (e.g., switch tabs with swipe)

ğŸ§¾ Table of Contents
Project Structure
Installation
Dataset Collection
Model Training
Running the Application
Usage
Model Architecture
License

ğŸ“‚ Project Structure

.
â”œâ”€â”€ app.py                  # Flask app to serve live video feed
â”œâ”€â”€ camera.py               # Handles frame capture and routing to recognizer/tracker
â”œâ”€â”€ data_collector.py       # Script for collecting labeled hand gesture images
â”œâ”€â”€ recognizer.py           # Gesture recognizer using CNN
â”œâ”€â”€ tracker.py              # Hand gesture tracker using MediaPipe
â”œâ”€â”€ train.py                # Training script for CNN model
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html          # Frontend HTML for Flask app
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ gesture-model.json      # Saved CNN model architecture
â”‚   â”œâ”€â”€ gesture-model.weights.h5 # Trained weights
â”‚   â””â”€â”€ training_history.png    # Training loss and accuracy graph
â””â”€â”€ data/
    â”œâ”€â”€ train/
    â””â”€â”€ test/               # Organized gesture images by label


    
ğŸ› ï¸ Installation
1. Clone the Repository
https://github.com/naqeebahmedkhan-code/REAL-TIME-GETURE-RECOGNITION-USING-3D-CNN-s.git
cd gesture-control
2. Create a Virtual Environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # On Windows use: venv\Scripts\activate
3. Install Dependencies
pip install -r requirements.txt
You may need to install opencv-python, tensorflow, mediapipe, flask, pyautogui, matplotlib, etc.

ğŸ¯ Dataset Collection
Run the following to collect training data for gestures:
python data_collector.py
Prompts for mode: train or test
Press:

0 â†’ move-right

1 â†’ move-left

2 â†’ fist

3 â†’ no-gesture

ESC to exit

Captured grayscale thresholded ROI images are saved in data/train/ or data/test/.

ğŸ§  Model Training

Train the CNN model using:
python train.py
Saves trained model in models/
Saves model weights and architecture
Outputs training accuracy/loss graph as training_history.png

ğŸš€ Running the Application
Start the web server and open a browser to visualize the live gesture detection:
python app.py
Then go to: http://127.0.0.1:5000/

Use the UI to see live feedback:

Default mode: CNN-based recognizer

Switch mode via /set_mode/tracker or /set_mode/cnn URL

ğŸ® Usage
Recognizer Mode (cnn):

Detects gestures inside a defined ROI using the trained model.

Tracker Mode (tracker):

Uses MediaPipe to identify gestures (e.g., number of fingers, swipe direction).

Automatically switches tabs using pyautogui.

ğŸ—ï¸ Model Architecture
CNN Model Summary:

Conv2D (32) â†’ MaxPool

Conv2D (64) â†’ MaxPool

Conv2D (128) â†’ MaxPool

Flatten â†’ Dense (256) â†’ Dropout â†’ Dense (4 - Softmax)

ğŸ“¸ Sample Gestures
Gesture	Keybind	Description
move-right	0	Swipe gesture to the right
move-left	1	Swipe gesture to the left
fist	2	Closed fist gesture
no-gesture	3	Neutral/no gesture shown

âœ… Requirements
Python 3.7+
TensorFlow 2.x
OpenCV
Flask
MediaPipe
PyAutoGUI
Matplotlib

ğŸ“„ License
This project is licensed under the MIT License - see the LICENSE file for details.

ğŸ™Œ Acknowledgements
TensorFlow
MediaPipe
OpenCV
Flask
